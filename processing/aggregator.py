import os
from typing import Dict, List, Optional

import numpy as np
import pandas as pd

from processing.utilities.category_mapping_loader import (
    get_default_category_value,
    map_category_series,
)


# =============================
# STEP1: é›†ç´„ï¼ˆåˆ¶å¾¡ã‚¨ãƒªã‚¢å˜ä½ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰
# =============================
class AreaAggregator:
    """åˆ¶å¾¡ã‚¨ãƒªã‚¢å˜ä½ã«ã€ç©ºèª¿ãƒ»é›»åŠ›ãƒ»å¤©å€™ã‚’1æ™‚é–“å˜ä½ã§çµ±åˆ"""

    def __init__(self, master_info: dict):
        self.m = master_info

    @staticmethod
    def _most_frequent(s: pd.Series):
        return s.mode().iloc[0] if not s.mode().empty else np.nan

    def build(
        self,
        ac: Optional[pd.DataFrame],
        pm: Optional[pd.DataFrame],
        weather: Optional[pd.DataFrame],
        freq: str = "1H",
        apply_zone_mapping: bool = True,
    ) -> pd.DataFrame:
        if self.m is None or "zones" not in self.m:
            raise ValueError("ãƒã‚¹ã‚¿ã« zones ãŒã‚ã‚Šã¾ã›ã‚“")
        zones = self.m["zones"]

        # å¤©å€™ï¼ˆå…±é€šï¼‰
        weather = weather.copy() if weather is not None else pd.DataFrame()
        if not weather.empty:
            # å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã®åˆ—åã‚’çµ±ä¸€ï¼ˆdatetime -> Datetimeï¼‰
            if "datetime" in weather.columns:
                weather["Datetime"] = pd.to_datetime(weather["datetime"]).dt.floor(
                    freq.replace("H", "h")
                )
            elif "Datetime" in weather.columns:
                weather["Datetime"] = pd.to_datetime(weather["Datetime"]).dt.floor(freq)
            else:
                print(
                    f"âš ï¸ å¤©æ°—ãƒ‡ãƒ¼ã‚¿ã«Datetimeåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(weather.columns)}"
                )
                return pd.DataFrame()
            wcols = [
                c
                for c in [
                    "Outdoor Temp.",
                    "Outdoor Humidity",
                    "Solar Radiation",
                    "temperature C",
                    "humidity",
                ]
                if c in weather.columns
            ]
            weather = (
                weather[["Datetime"] + wcols]
                .groupby("Datetime")
                .agg("mean")
                .reset_index()
            )
            # åˆ—åçµ±ä¸€
            if (
                "temperature C" in weather.columns
                and "Outdoor Temp." not in weather.columns
            ):
                weather.rename(columns={"temperature C": "Outdoor Temp."}, inplace=True)
            if (
                "humidity" in weather.columns
                and "Outdoor Humidity" not in weather.columns
            ):
                weather.rename(columns={"humidity": "Outdoor Humidity"}, inplace=True)

        # åˆ¶å¾¡ã‚¨ãƒªã‚¢ã”ã¨ã«ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹ç¯‰
        area_rows = []
        for zone_name, zinfo in zones.items():
            # å®¤å†…æ©Ÿä¸€è¦§
            indoor_units: List[str] = []
            # å®¤å¤–æ©Ÿ: {id: {load_share: x}}
            outdoor_units: Dict[str, dict] = zinfo.get("outdoor_units", {})
            for _, ou in outdoor_units.items():
                indoor_units.extend(ou.get("indoor_units", []))
            indoor_units = list(dict.fromkeys(indoor_units))  # unique & keep order

            # ç©ºèª¿ï¼ˆå®¤å†…æ©Ÿï¼‰: 1æ™‚é–“ã”ã¨ æœ€é »å€¤/å¹³å‡
            if ac is not None and not ac.empty and indoor_units:
                ac_sub = ac[ac["A/C Name"].isin(indoor_units)].copy()
                if not ac_sub.empty:
                    # ã‚¨ãƒªã‚¢åˆ¥ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨
                    if apply_zone_mapping:
                        ac_sub = self._apply_zone_categorical_mapping(ac_sub, zone_name)

                    ac_sub["Datetime"] = pd.to_datetime(ac_sub["Datetime"]).dt.floor(
                        freq.replace("H", "h")
                    )
                    # After categorical mapping, A/C ON/OFF is already numeric (0=OFF, 1=ON)
                    # So we can use it directly for counting units ON

                    g = (
                        ac_sub.groupby("Datetime")
                        .agg(
                            {
                                # TODO: Avgã«å¤‰æ›´ã™ã‚€è‰¯ã„ã‹ã©ã†ã‹æ¤œè¨ãŒå¿…è¦ã€‚å„ªå…ˆåº¦ä½ã„
                                "A/C Set Temperature": AreaAggregator._most_frequent,
                                "Indoor Temp.": "mean",  # å­¦ç¿’ã¯å¹³å‡å®¤æ¸©
                                "A/C ON/OFF": "sum",  # Count of units ON
                                "A/C Mode": AreaAggregator._most_frequent,
                                "A/C Fan Speed": AreaAggregator._most_frequent,
                            }
                        )
                        .reset_index()
                    )

                    # Create A/C Status column based on ON/OFF count and Mode
                    # Status mapping: OFF=0, COOL=1, HEAT=2, FAN=3
                    if "A/C ON/OFF" in g.columns and "A/C Mode" in g.columns:
                        g["A/C Status"] = 0  # Default to OFF
                        # If any units are ON, use the mode value
                        on_mask = g["A/C ON/OFF"] > 0
                        g.loc[on_mask, "A/C Status"] = g.loc[
                            on_mask, "A/C Mode"
                        ].astype(int)
                        # Convert to integer type (not float)
                        g["A/C Status"] = g["A/C Status"].fillna(0).astype(int)
                        print(
                            f"[AreaAggregator] Zone {zone_name}: Created A/C Status column"
                        )
                else:
                    g = pd.DataFrame(
                        columns=[
                            "Datetime",
                            "A/C Set Temperature",
                            "Indoor Temp.",
                            "A/C ON/OFF",
                            "A/C Mode",
                            "A/C Fan Speed",
                            "A/C Status",
                        ]
                    )
            else:
                g = pd.DataFrame(
                    columns=[
                        "Datetime",
                        "A/C Set Temperature",
                        "Indoor Temp.",
                        "A/C ON/OFF",
                        "A/C Mode",
                        "A/C Fan Speed",
                        "A/C Status",
                    ]
                )

            # é›»åŠ›ï¼ˆå®¤å¤–æ©ŸÃ—è² è·ç‡ã®åˆè¨ˆï¼‰
            p_list = []
            if pm is not None and not pm.empty and outdoor_units:
                print(
                    f"[AreaAggregator] Zone {zone_name}: Processing {len(outdoor_units)} outdoor units"
                )

                for ou_id, ou in outdoor_units.items():
                    share = float(ou.get("load_share", 1.0))

                    # Try exact match first
                    sub = pm[pm["Mesh ID"] == ou_id].copy()

                    # If no exact match, try extracting the base number (e.g., "49-1" -> 49)
                    if sub.empty and "-" in str(ou_id):
                        base_id = int(str(ou_id).split("-")[0])
                        sub = pm[pm["Mesh ID"] == base_id].copy()

                    if sub.empty:
                        continue

                    print(
                        f"[AreaAggregator] Found {len(sub)} records for Mesh ID: {ou_id}"
                    )

                    # Total_kWhåˆ—ã®å­˜åœ¨ç¢ºèª
                    if "Total_kWh" not in sub.columns:
                        print(
                            f"âš ï¸ Total_kWhåˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(sub.columns)}"
                        )
                        if "Phase A" in sub.columns:
                            print(f"  Phase Aåˆ—ã‚’ä½¿ç”¨ã—ã¾ã™")
                            sub["Total_kWh"] = sub["Phase A"]
                        else:
                            print(f"  âŒ é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                            continue

                    sub["Datetime"] = pd.to_datetime(sub["Datetime"]).dt.floor(
                        freq.replace("H", "h")
                    )
                    sub = sub.groupby("Datetime")["Total_kWh"].sum().reset_index()
                    sub["adjusted_power"] = sub["Total_kWh"] * share

                    print(
                        f"  Total_kWhçµ±è¨ˆ: å¹³å‡={sub['Total_kWh'].mean():.2f}, æœ€å¤§={sub['Total_kWh'].max():.2f}"
                    )
                    print(
                        f"  adjusted_powerçµ±è¨ˆ: å¹³å‡={sub['adjusted_power'].mean():.2f}, æœ€å¤§={sub['adjusted_power'].max():.2f}"
                    )

                    p_list.append(sub[["Datetime", "adjusted_power"]])
            if p_list:
                p = (
                    pd.concat(p_list, ignore_index=True)
                    .groupby("Datetime")["adjusted_power"]
                    .sum()
                    .reset_index()
                )

                print(f"[AreaAggregator] é›»åŠ›ãƒ‡ãƒ¼ã‚¿çµ±åˆçµæœ:")
                print(f"  çµ±åˆå‰ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(p_list)}")
                print(f"  çµ±åˆå¾Œãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(p)}")
                print(f"  adjusted_poweræ¬ æå€¤: {p['adjusted_power'].isnull().sum()}ä»¶")
                print(
                    f"  adjusted_powerçµ±è¨ˆ: å¹³å‡={p['adjusted_power'].mean():.2f}, æœ€å¤§={p['adjusted_power'].max():.2f}"
                )
            else:
                p = pd.DataFrame(columns=["Datetime", "adjusted_power"])
                print(f"[AreaAggregator] é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # ãƒãƒ¼ã‚¸
            df = g.merge(p, on="Datetime", how="outer")

            print(f"[AreaAggregator] ãƒãƒ¼ã‚¸å¾Œ:")
            print(f"  dfãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df)}")
            print(f"  adjusted_poweræ¬ æå€¤: {df['adjusted_power'].isnull().sum()}ä»¶")
            if not weather.empty:
                df = df.merge(weather, on="Datetime", how="left")

            # adjusted_powerã®æ¬ æå€¤åˆ†æ
            missing_power = df["adjusted_power"].isnull().sum()
            if missing_power > 0:
                print(f"âš ï¸ adjusted_powerã«æ¬ æå€¤ãŒ{missing_power}ä»¶ã‚ã‚Šã¾ã™")

                # æ¬ æå€¤ã®åŸå› åˆ†æ
                missing_df = df[df["adjusted_power"].isnull()].copy()
                print(f"  æ¬ æå€¤ã®è©³ç´°åˆ†æ:")
                print(f"    æ¬ æãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(missing_df)}")

                # æ™‚é–“ç¯„å›²ã®ç¢ºèª
                if not missing_df.empty:
                    print(
                        f"    æ¬ ææœŸé–“: {missing_df['Datetime'].min()} ï½ {missing_df['Datetime'].max()}"
                    )

                    # é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    has_power_data = (
                        "adjusted_power" in df.columns
                        and not df["adjusted_power"].isnull().all()
                    )
                    if has_power_data:
                        non_missing_count = df["adjusted_power"].notnull().sum()
                        print(f"    é›»åŠ›ãƒ‡ãƒ¼ã‚¿å­˜åœ¨: {non_missing_count}ä»¶")
                        print(
                            f"    é›»åŠ›ãƒ‡ãƒ¼ã‚¿æ¬ æç‡: {missing_power / len(df) * 100:.1f}%"
                        )
                    else:
                        print(f"    âŒ é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãŒå…¨ãå­˜åœ¨ã—ã¾ã›ã‚“")

                    # ç©ºèª¿ãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒ
                    if "Indoor Temp." in df.columns:
                        temp_missing = df["Indoor Temp."].isnull().sum()
                        print(f"    å®¤æ¸©ãƒ‡ãƒ¼ã‚¿æ¬ æ: {temp_missing}ä»¶")
                        if temp_missing == 0:
                            print(f"    âš ï¸ å®¤æ¸©ãƒ‡ãƒ¼ã‚¿ã¯å­˜åœ¨ã™ã‚‹ãŒé›»åŠ›ãƒ‡ãƒ¼ã‚¿ãŒæ¬ æ")
                        else:
                            print(f"    âš ï¸ å®¤æ¸©ãƒ‡ãƒ¼ã‚¿ã‚‚æ¬ æã—ã¦ã„ã‚‹å¯èƒ½æ€§")

                # é›»åŠ›ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆå‰å¾Œã®çŠ¶æ³ç¢ºèª
                if p_list:
                    print(f"  é›»åŠ›ãƒ‡ãƒ¼ã‚¿çµ±åˆå‰ã®çŠ¶æ³:")
                    print(f"    çµ±åˆå‰ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(p_list)}")
                    print(f"    çµ±åˆå¾Œãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(p)}")
                    print(f"    çµ±åˆå¾Œæ¬ æå€¤: {p['adjusted_power'].isnull().sum()}ä»¶")
                else:
                    print(f"    âŒ é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãŒçµ±åˆã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆp_listãŒç©ºï¼‰")

                # ãƒãƒ¼ã‚¸ã®çŠ¶æ³ç¢ºèª
                print(f"  ãƒãƒ¼ã‚¸çŠ¶æ³:")
                print(f"    ç©ºèª¿ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(g)}")
                print(f"    é›»åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(p)}")
                print(f"    ãƒãƒ¼ã‚¸å¾Œãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df)}")

                # æ™‚é–“ç¯„å›²ã®é‡è¤‡ç¢ºèª
                if not g.empty and not p.empty:
                    g_time_range = (g["Datetime"].min(), g["Datetime"].max())
                    p_time_range = (p["Datetime"].min(), p["Datetime"].max())
                    print(
                        f"    ç©ºèª¿ãƒ‡ãƒ¼ã‚¿æ™‚é–“ç¯„å›²: {g_time_range[0]} ï½ {g_time_range[1]}"
                    )
                    print(
                        f"    é›»åŠ›ãƒ‡ãƒ¼ã‚¿æ™‚é–“ç¯„å›²: {p_time_range[0]} ï½ {p_time_range[1]}"
                    )

                    # æ™‚é–“ç¯„å›²ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    overlap_start = max(g_time_range[0], p_time_range[0])
                    overlap_end = min(g_time_range[1], p_time_range[1])
                    if overlap_start <= overlap_end:
                        print(
                            f"    âœ… æ™‚é–“ç¯„å›²ã«é‡è¤‡ãŒã‚ã‚Šã¾ã™: {overlap_start} ï½ {overlap_end}"
                        )
                    else:
                        print(f"    âŒ æ™‚é–“ç¯„å›²ã«é‡è¤‡ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                print(f"âœ… adjusted_powerã«æ¬ æå€¤ã¯ã‚ã‚Šã¾ã›ã‚“")
                print(
                    f"  adjusted_powerçµ±è¨ˆ: å¹³å‡={df['adjusted_power'].mean():.2f}, æœ€å¤§={df['adjusted_power'].max():.2f}"
                )

            df["zone"] = zone_name
            df.sort_values(
                "Datetime", ascending=False, inplace=True
            )  # Sort by latest first (newest to oldest)
            area_rows.append(df)

        area_df = (
            pd.concat(area_rows, ignore_index=True) if area_rows else pd.DataFrame()
        )
        # ãƒ©ã‚°ï¼ˆå‰æ™‚åˆ»å®¤æ¸©ï¼‰
        if not area_df.empty:
            # Sort the final concatenated dataframe by Datetime in descending order (newest to oldest)
            area_df.sort_values("Datetime", ascending=False, inplace=True)
            # æ™‚é–“ç‰¹å¾´é‡ã®ä»˜ä¸ï¼ˆæ›œæ—¥ãƒ»æ™‚åˆ»ãƒ»æœˆãƒ»é€±æœ«ï¼‰
            area_df["Datetime"] = pd.to_datetime(area_df["Datetime"])  # å®‰å…¨åŒ–
            area_df["Date"] = area_df["Datetime"].dt.date
            area_df["DayOfWeek"] = area_df["Datetime"].dt.dayofweek.astype(int)
            area_df["Hour"] = area_df["Datetime"].dt.hour.astype(int)
            area_df["Month"] = area_df["Datetime"].dt.month.astype(int)
            area_df["IsWeekend"] = area_df["DayOfWeek"].isin([5, 6]).astype(int)
            # ç¥æ—¥ãƒ•ãƒ©ã‚°ï¼ˆjpholidayãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ä½¿ç”¨ã€ãªã‘ã‚Œã°0ï¼‰
            try:
                import jpholiday  # type: ignore

                area_df["IsHoliday"] = (
                    area_df["Datetime"]
                    .dt.date.map(lambda d: 1 if jpholiday.is_holiday(d) else 0)
                    .astype(int)
                )
            except Exception:
                area_df["IsHoliday"] = 0
            area_df["Indoor Temp. Lag1"] = (
                area_df.sort_values(
                    ["zone", "Datetime"], ascending=[True, True]
                )  # Sort zones ascending, datetime ascending for lag calculation
                .groupby("zone")["Indoor Temp."]
                .shift(1)
            )
            area_df["Indoor Temp. Lag1"] = area_df["Indoor Temp. Lag1"].fillna(
                area_df["Indoor Temp."]
            )

            # æ¸©åº¦ã‚’å°æ•°ç‚¹ç¬¬1ä½ã«ä¸¸ã‚ã‚‹
            if "Indoor Temp." in area_df.columns:
                area_df["Indoor Temp."] = area_df["Indoor Temp."].round(1)
            if "Indoor Temp. Lag1" in area_df.columns:
                area_df["Indoor Temp. Lag1"] = area_df["Indoor Temp. Lag1"].round(1)
            if "Outdoor Temp." in area_df.columns:
                area_df["Outdoor Temp."] = area_df["Outdoor Temp."].round(1)

            # åˆ—ã®ä¸¦ã³é †ã‚’èª¿æ•´ï¼ˆDatetime, Dateã‚’æœ€åˆã«é…ç½®ï¼‰
            cols = list(area_df.columns)
            if "Datetime" in cols:
                cols.remove("Datetime")
            if "Date" in cols:
                cols.remove("Date")

            # Datetime, Dateã‚’æœ€åˆã«é…ç½®
            area_df = area_df[["Datetime", "Date"] + cols]

            # Final sort by Datetime in descending order (newest to oldest) after all processing
            area_df.sort_values("Datetime", ascending=False, inplace=True)

        return area_df

    def _apply_zone_categorical_mapping(
        self, dataframe: pd.DataFrame, zone_name: str
    ) -> pd.DataFrame:
        """ã‚¨ãƒªã‚¢åˆ¥ã®ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨"""
        import json
        import os
        from datetime import datetime

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™
        log_dir = f"logs/preprocessing/{self.m.get('store_name', 'unknown')}"
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(
            log_dir, f"zone_mapping_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )

        # ã‚¨ãƒªã‚¢åˆ¥ã®ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ­ã‚°
        zone_mapping_log = {
            "store_name": self.m.get("store_name", "unknown"),
            "timestamp": datetime.now().isoformat(),
            "zones": {},
        }

        print(f"\n[AreaAggregator] ã‚¨ãƒªã‚¢ '{zone_name}' ã®ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°å‡¦ç†é–‹å§‹")
        zone_log = {
            "zone_name": zone_name,
            "total_records": len(dataframe),
            "categorical_mappings": {},
        }

        # å„ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã‚’å‡¦ç†
        for column in ["A/C ON/OFF", "A/C Mode", "A/C Fan Speed"]:
            if column in dataframe.columns:
                print(f"[AreaAggregator] {zone_name} - {column} å‡¦ç†ä¸­...")

                # ã‚¨ãƒªã‚¢å›ºæœ‰ã®å€¤ã®åˆ†æ
                unique_values = dataframe[column].value_counts()
                print(
                    f"[AreaAggregator] {zone_name} - {column} ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤: {unique_values.to_dict()}"
                )

                original_series = dataframe[column]
                mapped_series, applied_mapping, unmapped_values = map_category_series(
                    original_series, column
                )
                dataframe[column] = mapped_series

                zone_log_entry = {
                    "original_values": unique_values.to_dict(),
                    "mapping": applied_mapping,
                    "mapped_count": len(applied_mapping),
                    "unmapped_count": int(sum(unmapped_values.values())),
                }
                if unmapped_values:
                    zone_log_entry["unmapped_values"] = unmapped_values
                zone_log["categorical_mappings"][column] = zone_log_entry

                if unmapped_values:
                    print(
                        f"[AreaAggregator] {zone_name} - {column} ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œãªã‹ã£ãŸå€¤: {unmapped_values}"
                    )
                    unmapped_mask = mapped_series.isna() & original_series.notna()
                    default_value = get_default_category_value(column)
                    if default_value is not None:
                        dataframe.loc[unmapped_mask, column] = default_value
                        zone_log_entry["default_value"] = default_value
                        print(
                            f"[AreaAggregator] {zone_name} - {column} ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤({default_value})ã§ç½®æ›: {int(unmapped_mask.sum())}ä»¶"
                        )

                # TODO : need to revisit later
                # Ensure all NA values are handled before converting to integer
                if dataframe[column].isna().any():
                    default_value = get_default_category_value(column)
                    if default_value is not None:
                        dataframe[column] = dataframe[column].fillna(default_value)
                        print(
                            f"[AreaAggregator] {zone_name} - {column} æ®‹ã‚Šã®NAå€¤ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤({default_value})ã§ç½®æ›"
                        )
                    else:
                        # If no default value, use 0 as fallback
                        dataframe[column] = dataframe[column].fillna(0)
                        print(
                            f"[AreaAggregator] {zone_name} - {column} æ®‹ã‚Šã®NAå€¤ã‚’0ã§ç½®æ›"
                        )

                dataframe[column] = dataframe[column].astype(pd.Int64Dtype())

        zone_mapping_log["zones"][zone_name] = zone_log

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆå®‰å…¨ãªæ›¸ãè¾¼ã¿ï¼‰
        try:
            with open(log_file, "w", encoding="utf-8") as f:
                json.dump(zone_mapping_log, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            backup_file = log_file.replace(".json", "_backup.json")
            try:
                with open(backup_file, "w", encoding="utf-8") as f:
                    json.dump(zone_mapping_log, f, ensure_ascii=False, indent=2)
                print(f"ğŸ“‹ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜: {backup_file}")
            except Exception as backup_e:
                print(f"âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜ã‚‚å¤±æ•—: {backup_e}")

        print(f"\n[AreaAggregator] ã‚¨ãƒªã‚¢åˆ¥ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ­ã‚°ä¿å­˜: {log_file}")

        return dataframe


def _load_weather_forecast(
    start_date: str,
    end_date: str,
    plan_dir: str,
    weather_api_key: str,
    coordinates: str,
) -> Optional[pd.DataFrame]:
    """
    Load weather forecast from cached file if it exists, otherwise fetch from API

    Args:
        start_date: Start date in YYYY-MM-DD format
        end_date: End date in YYYY-MM-DD format
        weather_api_key: Weather API key for fetching data if cache is missing
        coordinates: Coordinates for weather API

    Returns:
        Weather DataFrame if file exists or can be fetched from API, None otherwise
    """
    # Generate weather forecast file path with date range in filename
    start_clean = start_date.replace("-", "")
    end_clean = end_date.replace("-", "")
    filename = f"weather_forecast_{start_clean}_{end_clean}.csv"
    forecast_path = os.path.join(plan_dir, filename)

    if os.path.exists(forecast_path):
        print(f"[Run] Loading cached weather forecast: {forecast_path}")
        try:
            weather_df = pd.read_csv(forecast_path)

            # Convert datetime column to datetime type if it exists
            if "datetime" in weather_df.columns:
                weather_df["datetime"] = pd.to_datetime(weather_df["datetime"])
                print(f"[Run] Converted datetime column to datetime type")

            print(f"[Run] Cached weather data loaded. Shape: {weather_df.shape}")
            return weather_df
        except Exception as e:
            print(f"[Run] Error loading cached weather data: {e}")
            return None
    else:
        print(f"[Run] No cached weather forecast found: {forecast_path}")

        # Try to fetch weather data from API if credentials are provided
        if weather_api_key and coordinates:
            print("[Run] APIã‹ã‚‰å¤©å€™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—...")
            try:
                from processing.utilities.weatherapi_client import (
                    VisualCrossingWeatherAPIDataFetcher,
                )

                weather_df = VisualCrossingWeatherAPIDataFetcher(
                    coordinates=coordinates,
                    start_date=start_date,
                    end_date=end_date,
                    unit="metric",
                    api_key=weather_api_key,
                ).fetch()
                if weather_df is not None:
                    _save_weather_forecast(weather_df, start_date, end_date, plan_dir)
                    print("[Run] å¤©å€™ãƒ‡ãƒ¼ã‚¿ã‚’APIã‹ã‚‰å–å¾—ã—ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ã¾ã—ãŸ")
                    return weather_df
                else:
                    print("[Run] APIã‹ã‚‰å¤©å€™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    return None
            except Exception as e:
                print(f"[Run] å¤©å€™ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                return None
        else:
            print("[Run] å¤©å€™ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆAPIã‚­ãƒ¼ã¾ãŸã¯åº§æ¨™ãŒæœªè¨­å®šï¼‰")
            return None


def _save_weather_forecast(
    weather_df: pd.DataFrame, start_date: str, end_date: str, plan_dir: str
) -> None:
    """
    Save weather forecast to cached file with date range in filename

    Args:
        weather_df: Weather DataFrame to save
        start_date: Start date in YYYY-MM-DD format
        end_date: End date in YYYY-MM-DD format
    """
    start_clean = start_date.replace("-", "")
    end_clean = end_date.replace("-", "")
    filename = f"weather_forecast_{start_clean}_{end_clean}.csv"

    forecast_path = os.path.join(plan_dir, filename)
    try:
        os.makedirs(os.path.dirname(forecast_path), exist_ok=True)
        weather_df.to_csv(forecast_path, index=False, encoding="utf-8-sig")
        print(f"[Run] Weather forecast cached to: {forecast_path}")
    except Exception as e:
        print(f"[Run] Error saving weather forecast: {e}")


def aggregation_runner(
    store_name: str,
    store_master_file: dict,
    freq: str = "1H",
):
    """
    é›†ç´„ã®ã¿ã‚’å®Ÿè¡Œ
    Weather data is automatically determined from preprocessed data.

    Args:
        store_name: åº—èˆ—å
        store_master_file: ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿
        freq: æ™‚é–“ç²’åº¦

    Returns:
        pd.DataFrame: é›†ç´„ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
    """
    if store_master_file is None:
        print("[Aggregate] ãƒã‚¹ã‚¿æœªèª­è¾¼")
        return None

    print("[Aggregate] é›†ç´„ã®ã¿å®Ÿè¡Œé–‹å§‹...")

    # Get coordinates from store_master_file
    coordinates = store_master_file.get("store_info", {}).get("coordinates")
    if coordinates is None:
        print(f"[Aggregate] ERROR: No coordinates found in master data")
        return None
    else:
        print(f"[Aggregate] Using coordinates from master data: {coordinates}")

    # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    from config.utils import get_data_path

    proc_dir = os.path.join(get_data_path("processed_data_path"), store_name)
    plan_dir = os.path.join(get_data_path("output_data_path"), store_name)
    ac_p = os.path.join(proc_dir, f"ac_control_processed_{store_name}.csv")
    pm_p = os.path.join(proc_dir, f"power_meter_processed_{store_name}.csv")
    weather_p = os.path.join(proc_dir, f"weather_processed_{store_name}.csv")
    ac_processed_data = pd.read_csv(ac_p) if os.path.exists(ac_p) else None
    pm_processed_data = pd.read_csv(pm_p) if os.path.exists(pm_p) else None
    historical_weather_data = (
        pd.read_csv(weather_p) if os.path.exists(weather_p) else None
    )

    if ac_processed_data is None or pm_processed_data is None:
        print("[Aggregate] å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None

    # Determine date range from preprocessed data (not from optimization parameters)
    # Use AC data datetime as primary, fallback to power meter data
    if not ac_processed_data.empty and "Datetime" in ac_processed_data.columns:
        ac_processed_data["Datetime"] = pd.to_datetime(ac_processed_data["Datetime"])
        data_start_date = ac_processed_data["Datetime"].min()
        data_end_date = ac_processed_data["Datetime"].max()
    elif not pm_processed_data.empty and "Datetime" in pm_processed_data.columns:
        pm_processed_data["Datetime"] = pd.to_datetime(pm_processed_data["Datetime"])
        data_start_date = pm_processed_data["Datetime"].min()
        data_end_date = pm_processed_data["Datetime"].max()
    else:
        print("[Aggregate] ERROR: No datetime data found in preprocessed files")
        return None

    print(
        f"[Aggregate] Data date range: {data_start_date.date()} to {data_end_date.date()}"
    )

    # If historical weather data exists, use it and ignore any API calls
    if historical_weather_data is not None and not historical_weather_data.empty:
        print("[Aggregate] Using existing historical weather data from CSV")
        combined_weather_df = historical_weather_data
    else:
        # No historical weather exists - this is an error since preprocessing should have created it
        print(
            "[Aggregate] ERROR: Historical weather data not found. "
            "Please run the preprocessor module first to generate weather data."
        )
        return None

    # é›†ç´„ã®å®Ÿè¡Œ
    # Use master data from constructor
    if store_master_file is None:
        print("[Aggregate] ERROR: Master data not available for aggregator")
        return None

    # Extract zones data for aggregator
    aggregator_data = {
        "store_name": store_master_file.get("store_info", {}).get("name", store_name),
        "zones": store_master_file.get("zones", {}),
    }
    aggregator = AreaAggregator(aggregator_data)
    area_df = aggregator.build(
        ac_processed_data, pm_processed_data, combined_weather_df, freq=freq
    )

    # ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
    if area_df is not None:
        area_out = os.path.join(proc_dir, f"features_processed_{store_name}.csv")
        os.makedirs(proc_dir, exist_ok=True)
        area_df.to_csv(area_out, index=False, encoding="utf-8-sig")
        print(f"[Aggregate] é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {area_out}")

    print("[Aggregate] é›†ç´„å®Œäº†")
    return area_df
