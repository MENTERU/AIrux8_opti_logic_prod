#!/usr/bin/env python3
"""
Cloud Run Job entrypoint - runs scraping directly
"""
import asyncio
import sys
from datetime import datetime, timedelta

import pytz
from service.airux8_scraper import Alrux8Scraper
from service.secretmanager import SecretManagerClient

# BigQuery dataset and table names for Clea scraping
BQ_DATASET_CLEA = "Clea"
BQ_TABLE_AC_CONTROL_RAW = "ac_control_raw"
BQ_TABLE_AC_POWER_METER_RAW = "ac_power_meter_raw"


async def main():
    """Main function to run scraping job

    Args:
        None

    Returns:
        None
    """
    scraper = None
    try:
        print("=" * 60)
        print("ğŸ“¥ Cloud Run Job: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
        print("=" * 60)

        # ãƒ­ã‚°ã‚¤ãƒ³æƒ…å ±ã‚’Secret Managerã‹ã‚‰å–å¾—
        secret_manager = SecretManagerClient()
        login_info = secret_manager.get_secret_as_dict("AIRUX8_WEB_LOGIN_INFO")

        if not login_info:
            print("âŒ Failed to retrieve login information from Secret Manager")
            sys.exit(1)

        username = login_info.get("username")
        password = login_info.get("password")

        if not username or not password:
            print("âŒ Login information is missing username or password")
            sys.exit(1)

        print("âœ… Successfully retrieved login information from Secret Manager")

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä½œæˆï¼ˆBigQueryãƒ†ãƒ¼ãƒ–ãƒ«è¨­å®šã‚’æ¸¡ã™ï¼‰
        scraper = Alrux8Scraper(
            bq_dataset_id=BQ_DATASET_CLEA,
            bq_table_ac_control_raw=BQ_TABLE_AC_CONTROL_RAW,
            bq_table_ac_power_meter_raw=BQ_TABLE_AC_POWER_METER_RAW,
        )
        store_name = "Clea"
        today = datetime.now(pytz.timezone("Asia/Tokyo"))
        yesterday_date = (today - timedelta(days=1)).date()
        # Set both start and end date to yesterday (date only, no time components)
        start_date = datetime(
            yesterday_date.year,
            yesterday_date.month,
            yesterday_date.day,
            tzinfo=pytz.timezone("Asia/Tokyo"),
        )
        end_date = datetime(
            yesterday_date.year,
            yesterday_date.month,
            yesterday_date.day,
            tzinfo=pytz.timezone("Asia/Tokyo"),
        )
        data_types = ["A/C Power Meter", "A/Cåˆ¶å¾¡"]

        print(f"=== {store_name} ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ ===")
        print(
            f"æœŸé–“: {start_date.strftime('%Y-%m-%d')} ï½ {end_date.strftime('%Y-%m-%d')}"
        )
        print(f"ãƒ‡ãƒ¼ã‚¿: {', '.join(data_types)}")

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        success = await scraper.run_scraping(
            username=username,
            password=password,
            store_name=store_name,
            start_date=start_date,
            end_date=end_date,
            data_types=data_types,
        )

        if success:
            print("=" * 60)
            print("âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
            print("=" * 60)
            sys.exit(0)
        else:
            print("=" * 60)
            print("âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")
            print("=" * 60)
            sys.exit(1)
    except Exception as error:
        print("=" * 60)
        print(f"âŒ ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {error}")
        import traceback

        traceback.print_exc()
        print("=" * 60)
        sys.exit(1)
    finally:
        if scraper is not None:
            await scraper.close()
        print("ğŸ”„ ã‚¸ãƒ§ãƒ–çµ‚äº†")


if __name__ == "__main__":
    asyncio.run(main())
